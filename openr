Quickstart


Copy page

Get started with OpenRouter

OpenRouter provides a unified API that gives you access to hundreds of AI models through a single endpoint, while automatically handling fallbacks and selecting the most cost-effective options. Get started with just a few lines of code using your preferred SDK or framework.

Looking for information about free models and rate limits? Please see the FAQ

In the examples below, the OpenRouter-specific headers are optional. Setting them allows your app to appear on the OpenRouter leaderboards.

Using the OpenAI SDK

Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
  defaultHeaders: {
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
  },
});
async function main() {
  const completion = await openai.chat.completions.create({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  });
  console.log(completion.choices[0].message);
}
main();
Using the OpenRouter API directly
You can use the interactive Request Builder to generate OpenRouter API requests in the language of your choice.


Python

TypeScript

Shell

fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  }),
});
The API also supports streaming.

Headers
OpenRouter allows you to specify some optional headers to identify your app and make it discoverable to users on our site.

HTTP-Referer: Identifies your app on openrouter.ai
X-Title: Sets/modifies your app’s title
TypeScript

fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  }),
});
Model routing
If the model parameter is omitted, the user or payer’s default is used. Otherwise, remember to select a value for model from the supported models or API, and include the organization prefix. OpenRouter will select the least expensive and best GPUs available to serve the request, and fall back to other providers or GPUs if it receives a 5xx response code or if you are rate-limited.

Streaming
Server-Sent Events (SSE) are supported as well, to enable streaming for all models. Simply send stream: true in your request body. The SSE stream will occasionally contain a “comment” payload, which you should ignore (noted below).

Non-standard parameters
If the chosen model doesn’t support a request parameter (such as logit_bias in non-OpenAI models, or top_k for OpenAI), then the parameter is ignored. The rest are forwarded to the underlying model API.

Assistant Prefill
OpenRouter supports asking models to complete a partial response. This can be useful for guiding models to respond in a certain way.

To use this features, simply include a message with role: "assistant" at the end of your messages array.

TypeScript

fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      { role: 'user', content: 'What is the meaning of life?' },
      { role: 'assistant', content: "I'm not sure, but my best guess is" },
    ],
  }),
});
Responses
CompletionsResponse Format
OpenRouter normalizes the schema across models and providers to comply with the OpenAI Chat API.

This means that choices is always an array, even if the model only returns one completion. Each choice will contain a delta property if a stream was requested and a message property otherwise. This makes it easier to use the same code for all models.

Here’s the response schema as a TypeScript type:

TypeScript

// Definitions of subtypes are below
type Response = {
  id: string;
  // Depending on whether you set "stream" to "true" and
  // whether you passed in "messages" or a "prompt", you
  // will get a different output shape
  choices: (NonStreamingChoice | StreamingChoice | NonChatChoice)[];
  created: number; // Unix timestamp
  model: string;
  object: 'chat.completion' | 'chat.completion.chunk';
  system_fingerprint?: string; // Only present if the provider supports it
  // Usage data is always returned for non-streaming.
  // When streaming, you will get one usage object at
  // the end accompanied by an empty choices array.
  usage?: ResponseUsage;
};
// If the provider returns usage, we pass it down
// as-is. Otherwise, we count using the GPT-4 tokenizer.
type ResponseUsage = {
  /** Including images and tools if any */
  prompt_tokens: number;
  /** The tokens generated */
  completion_tokens: number;
  /** Sum of the above two fields */
  total_tokens: number;
};

// Subtypes:
type NonChatChoice = {
  finish_reason: string | null;
  text: string;
  error?: ErrorResponse;
};
type NonStreamingChoice = {
  finish_reason: string | null;
  native_finish_reason: string | null;
  message: {
    content: string | null;
    role: string;
    tool_calls?: ToolCall[];
  };
  error?: ErrorResponse;
};
type StreamingChoice = {
  finish_reason: string | null;
  native_finish_reason: string | null;
  delta: {
    content: string | null;
    role?: string;
    tool_calls?: ToolCall[];
  };
  error?: ErrorResponse;
};
type ErrorResponse = {
  code: number; // See "Error Handling" section
  message: string;
  metadata?: Record<string, unknown>; // Contains additional error information such as provider details, the raw error message, etc.
};
type ToolCall = {
  id: string;
  type: 'function';
  function: FunctionCall;
};

Here’s an example:

{
  "id": "gen-xxxxxxxxxxxxxx",
  "choices": [
    {
      "finish_reason": "stop", // Normalized finish_reason
      "native_finish_reason": "stop", // The raw finish_reason from the provider
      "message": {
        // will be "delta" if streaming
        "role": "assistant",
        "content": "Hello there!"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 4,
    "total_tokens": 4
  },
  "model": "openai/gpt-3.5-turbo" // Could also be "anthropic/claude-2.1", etc, depending on the "model" that ends up being used
}

Finish Reason
OpenRouter normalizes each model’s finish_reason to one of the following values: tool_calls, stop, length, content_filter, error.

Some models and providers may have additional finish reasons. The raw finish_reason string returned by the model is available via the native_finish_reason property.

Querying Cost and Stats
The token counts that are returned in the completions API response are not counted via the model’s native tokenizer. Instead it uses a normalized, model-agnostic count (accomplished via the GPT4o tokenizer). This is because some providers do not reliably return native token counts. This behavior is becoming more rare, however, and we may add native token counts to the response object in the future.

Credit usage and model pricing are based on the native token counts (not the ‘normalized’ token counts returned in the API response).

For precise token accounting using the model’s native tokenizer, you can retrieve the full generation information via the /api/v1/generation endpoint.

You can use the returned id to query for the generation stats (including token counts and cost) after the request is complete. This is how you can get the cost and tokens for all models and requests, streaming and non-streaming.

Query Generation Stats

const generation = await fetch(
  'https://openrouter.ai/api/v1/generation?id=$GENERATION_ID',
  { headers },
);
const stats = await generation.json();
Please see the Generation API reference for the full response shape.

Note that token counts are also available in the usage field of the response body for non-streaming completions.

Streaming


Copy page

The OpenRouter API allows streaming responses from any model. This is useful for building chat interfaces or other applications where the UI should update as the model generates the response.

To enable streaming, you can set the stream parameter to true in your request. The model will then stream the response to the client in chunks, rather than returning the entire response at once.

Here is an example of how to stream a response, and process it:


Python

TypeScript

const question = 'How would you build the tallest building ever?';
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: `Bearer ${API_KEY_REF}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [{ role: 'user', content: question }],
    stream: true,
  }),
});
const reader = response.body?.getReader();
if (!reader) {
  throw new Error('Response body is not readable');
}
const decoder = new TextDecoder();
let buffer = '';
try {
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    // Append new chunk to buffer
    buffer += decoder.decode(value, { stream: true });
    // Process complete lines from buffer
    while (true) {
      const lineEnd = buffer.indexOf('\n');
      if (lineEnd === -1) break;
      const line = buffer.slice(0, lineEnd).trim();
      buffer = buffer.slice(lineEnd + 1);
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') break;
        try {
          const parsed = JSON.parse(data);
          const content = parsed.choices[0].delta.content;
          if (content) {
            console.log(content);
          }
        } catch (e) {
          // Ignore invalid JSON
        }
      }
    }
  }
} finally {
  reader.cancel();
}
Additional Information
For SSE (Server-Sent Events) streams, OpenRouter occasionally sends comments to prevent connection timeouts. These comments look like:

: OPENROUTER PROCESSING

Comment payload can be safely ignored per the SSE specs. However, you can leverage it to improve UX as needed, e.g. by showing a dynamic loading indicator.

Some SSE client implementations might not parse the payload according to spec, which leads to an uncaught error when you JSON.stringify the non-JSON payloads. We recommend the following clients:

eventsource-parser
OpenAI SDK
Vercel AI SDK
Stream Cancellation
Streaming requests can be cancelled by aborting the connection. For supported providers, this immediately stops model processing and billing.

Provider Support
To implement stream cancellation:


Python

TypeScript

const controller = new AbortController();
try {
  const response = await fetch(
    'https://openrouter.ai/api/v1/chat/completions',
    {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${<OPENROUTER_API_KEY>}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'openai/gpt-4o',
        messages: [{ role: 'user', content: 'Write a story' }],
        stream: true,
      }),
      signal: controller.signal,
    },
  );
  // Process the stream...
} catch (error) {
  if (error.name === 'AbortError') {
    console.log('Stream cancelled');
  } else {
    throw error;
  }
}
// To cancel the stream:
controller.abort();
Cancellation only works for streaming requests with supported providers. For non-streaming requests or unsupported providers, the model will continue processing and you will be billed for the complete response.

Limits


Copy page

Rate Limits

Making additional accounts or API keys will not affect your rate limits, as we govern capacity globally. We do however have different rate limits for different models, so you can share the load that way if you do run into issues.

If you have a consistent need for high rate limits (>2000 RPM) — email us.
Rate Limits and Credits Remaining
To check the rate limit or credits left on an API key, make a GET request to https://openrouter.ai/api/v1/auth/key.


TypeScript

Python

const response = await fetch('https://openrouter.ai/api/v1/auth/key', {
  method: 'GET',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
  },
});
If you submit a valid API key, you should get a response of the form:

TypeScript

type Key = {
  data: {
    label: string;
    usage: number; // Number of credits used
    limit: number | null; // Credit limit for the key, or null if unlimited
    is_free_tier: boolean; // Whether the user has paid for credits before
    rate_limit: {
      requests: number; // Number of requests allowed...
      interval: string; // in this interval, e.g. "10s"
    };
  };
};
There are a few rate limits that apply to certain types of requests, regardless of account status:

Free usage limits: If you’re using a free model variant (with an ID ending in :free), you can make up to 20 requests per minute. The following per-day limits apply:
If you have purchased less than 10 credits, you’re limited to 50 :free model requests per day.

If you purchase at least 10 credits, your daily limit is increased to 1000 :free model requests per day.

DDoS protection: Cloudflare’s DDoS protection will block requests that dramatically exceed reasonable usage.
If your account has a negative credit balance, you may see 402 errors, including for free models. Adding credits to put your balance above zero allows you to use those models again.

Authentication


Copy page

API Authentication

You can cover model costs with OpenRouter API keys.

Our API authenticates requests using Bearer tokens. This allows you to use curl or the OpenAI SDK directly with OpenRouter.

API keys on OpenRouter are more powerful than keys used directly for model APIs.

They allow users to set credit limits for apps, and they can be used in OAuth flows.

Using an API key
To use an API key, first create your key. Give it a name and you can optionally set a credit limit.

If you’re calling the OpenRouter API directly, set the Authorization header to a Bearer token with your API key.

If you’re using the OpenAI Typescript SDK, set the api_base to https://openrouter.ai/api/v1 and the apiKey to your API key.


TypeScript (Bearer Token)

TypeScript (OpenAI SDK)

Python

Shell

fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  }),
});
To stream with Python, see this example from OpenAI.

If your key has been exposed
You must protect your API keys and never commit them to public repositories.

OpenRouter is a GitHub secret scanning partner, and has other methods to detect exposed keys. If we determine that your key has been compromised, you will receive an email notification.

If you receive such a notification or suspect your key has been exposed, immediately visit your key settings page to delete the compromised key and create a new one.

Using environment variables and keeping keys out of your codebase is strongly recommended.

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
  defaultHeaders: {
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
  },
});
async function main() {
  const completion = await openai.chat.completions.create({
    model: 'openai/gpt-4o',
    messages: [{ role: 'user', content: 'Say this is a test' }],
  });
  console.log(completion.choices[0].message);
}
Parameters


Copy page

Sampling parameters shape the token generation process of the model. You may send any parameters from the following list, as well as others, to OpenRouter.

OpenRouter will default to the values listed below if certain parameters are absent from your request (for example, temperature to 1.0). We will also transmit some provider-specific parameters, such as safe_prompt for Mistral or raw_mode for Hyperbolic directly to the respective providers if specified.

Please refer to the model’s provider section to confirm which parameters are supported. For detailed guidance on managing provider-specific parameters, click here.

Temperature
Key: temperature

Optional, float, 0.0 to 2.0

Default: 1.0

Explainer Video: Watch

This setting influences the variety in the model’s responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.

Top P
Key: top_p

Optional, float, 0.0 to 1.0

Default: 1.0

Explainer Video: Watch

This setting limits the model’s choices to a percentage of likely tokens: only the top tokens whose probabilities add up to P. A lower value makes the model’s responses more predictable, while the default setting allows for a full range of token choices. Think of it like a dynamic Top-K.

Top K
Key: top_k

Optional, integer, 0 or above

Default: 0

Explainer Video: Watch

This limits the model’s choice of tokens at each step, making it choose from a smaller set. A value of 1 means the model will always pick the most likely next token, leading to predictable results. By default this setting is disabled, making the model to consider all choices.

Frequency Penalty
Key: frequency_penalty

Optional, float, -2.0 to 2.0

Default: 0.0

Explainer Video: Watch

This setting aims to control the repetition of tokens based on how often they appear in the input. It tries to use less frequently those tokens that appear more in the input, proportional to how frequently they occur. Token penalty scales with the number of occurrences. Negative values will encourage token reuse.

Presence Penalty
Key: presence_penalty

Optional, float, -2.0 to 2.0

Default: 0.0

Explainer Video: Watch

Adjusts how often the model repeats specific tokens already used in the input. Higher values make such repetition less likely, while negative values do the opposite. Token penalty does not scale with the number of occurrences. Negative values will encourage token reuse.

Repetition Penalty
Key: repetition_penalty

Optional, float, 0.0 to 2.0

Default: 1.0

Explainer Video: Watch

Helps to reduce the repetition of tokens from the input. A higher value makes the model less likely to repeat tokens, but too high a value can make the output less coherent (often with run-on sentences that lack small words). Token penalty scales based on original token’s probability.

Min P
Key: min_p

Optional, float, 0.0 to 1.0

Default: 0.0

Represents the minimum probability for a token to be considered, relative to the probability of the most likely token. (The value changes depending on the confidence level of the most probable token.) If your Min-P is set to 0.1, that means it will only allow for tokens that are at least 1/10th as probable as the best possible option.

Top A
Key: top_a

Optional, float, 0.0 to 1.0

Default: 0.0

Consider only the top tokens with “sufficiently high” probabilities based on the probability of the most likely token. Think of it like a dynamic Top-P. A lower Top-A value focuses the choices based on the highest probability token but with a narrower scope. A higher Top-A value does not necessarily affect the creativity of the output, but rather refines the filtering process based on the maximum probability.

Seed
Key: seed

Optional, integer

If specified, the inferencing will sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed for some models.

Max Tokens
Key: max_tokens

Optional, integer, 1 or above

This sets the upper limit for the number of tokens the model can generate in response. It won’t produce more than this limit. The maximum value is the context length minus the prompt length.

Logit Bias
Key: logit_bias

Optional, map

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

Logprobs
Key: logprobs

Optional, boolean

Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned.

Top Logprobs
Key: top_logprobs

Optional, integer

An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.

Response Format
Key: response_format

Optional, map

Forces the model to produce specific output format. Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.

Note: when using JSON mode, you should also instruct the model to produce JSON yourself via a system or user message.

Structured Outputs
Key: structured_outputs

Optional, boolean

If the model can return structured outputs using response_format json_schema.

Stop
Key: stop

Optional, array

Stop generation immediately if the model encounter any token specified in the stop array.

Tools
Key: tools

Optional, array

Tool calling parameter, following OpenAI’s tool calling request shape. For non-OpenAI providers, it will be transformed accordingly. Click here to learn more about tool calling

Tool Choice
Key: tool_choice

Optional, array

Controls which (if any) tool is called by the model. ‘none’ means the model will not call any tool and instead generates a message. ‘auto’ means the model can pick between generating a message or calling one or more tools. ‘required’ means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool.

Errors


Copy page

API Errors

For errors, OpenRouter returns a JSON response with the following shape:

type ErrorResponse = {
  error: {
    code: number;
    message: string;
    metadata?: Record<string, unknown>;
  };
};

The HTTP Response will have the same status code as error.code, forming a request error if:

Your original request is invalid
Your API key/account is out of credits
Otherwise, the returned HTTP response status will be 200 and any error occurred while the LLM is producing the output will be emitted in the response body or as an SSE data event.

Example code for printing errors in JavaScript:

const request = await fetch('https://openrouter.ai/...');
console.log(request.status); // Will be an error code unless the model started processing your request
const response = await request.json();
console.error(response.error?.status); // Will be an error code
console.error(response.error?.message);

Error Codes
400: Bad Request (invalid or missing params, CORS)
401: Invalid credentials (OAuth session expired, disabled/invalid API key)
402: Your account or API key has insufficient credits. Add more credits and retry the request.
403: Your chosen model requires moderation and your input was flagged
408: Your request timed out
429: You are being rate limited
502: Your chosen model is down or we received an invalid response from it
503: There is no available model provider that meets your routing requirements
Moderation Errors
If your input was flagged, the error.metadata will contain information about the issue. The shape of the metadata is as follows:

type ModerationErrorMetadata = {
  reasons: string[]; // Why your input was flagged
  flagged_input: string; // The text segment that was flagged, limited to 100 characters. If the flagged input is longer than 100 characters, it will be truncated in the middle and replaced with ...
  provider_name: string; // The name of the provider that requested moderation
  model_slug: string;
};

Provider Errors
If the model provider encounters an error, the error.metadata will contain information about the issue. The shape of the metadata is as follows:

type ProviderErrorMetadata = {
  provider_name: string; // The name of the provider that encountered the error
  raw: unknown; // The raw error from the provider
};

When No Content is Generated
Occasionally, the model may not generate any content. This typically occurs when:

The model is warming up from a cold start
The system is scaling up to handle more requests
Warm-up times usually range from a few seconds to a few minutes, depending on the model and provider.

If you encounter persistent no-content issues, consider implementing a simple retry mechanism or trying again with a different provider or model that has more recent activity.

Additionally, be aware that in some cases, you may still be charged for the prompt processing cost by the upstream provider, even if no content is generated.

Was this page helpful?
Yes

Completion

POST
https://openrouter.ai/api/v1/completions
POST
/api/v1/completions

JavaScript

const url = 'https://openrouter.ai/api/v1/completions';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"model":"model","prompt":"prompt"}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Successful

{
  "id": "id",
  "choices": [
    {
      "text": "text",
      "index": 1,
      "finish_reason": "finish_reason"
    }
  ]
}
Send a completion request to a selected model (text-only format)

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Request
This endpoint expects an object.
model
string
Required
The model ID to use. If unspecified, the user’s default is used.

prompt
string
Required
The text prompt to complete

models
list of strings
Optional
Alternate list of models for routing overrides.

provider
object
Optional
Preferences for provider routing.


Show 1 properties
reasoning
object
Optional
Configuration for model reasoning/thinking tokens


Show 3 properties
usage
object
Optional
Whether to include usage information in the response


Show 1 properties
transforms
list of strings
Optional
List of prompt transforms (OpenRouter-only).

stream
boolean
Optional
Defaults to false
Enable streaming of results.

max_tokens
integer
Optional
Maximum number of tokens (range: [1, context_length)).

temperature
double
Optional
Sampling temperature (range: [0, 2]).

seed
integer
Optional
Seed for deterministic outputs.

top_p
double
Optional
Top-p sampling value (range: (0, 1]).

top_k
integer
Optional
Top-k sampling value (range: [1, Infinity)).

frequency_penalty
double
Optional
Frequency penalty (range: [-2, 2]).

presence_penalty
double
Optional
Presence penalty (range: [-2, 2]).

repetition_penalty
double
Optional
Repetition penalty (range: (0, 2]).

logit_bias
map from strings to doubles
Optional
Mapping of token IDs to bias values.

top_logprobs
integer
Optional
Number of top log probabilities to return.

min_p
double
Optional
Minimum probability threshold (range: [0, 1]).

top_a
double
Optional
Alternate top sampling parameter (range: [0, 1]).

Response
Successful completion

id
string
Optional
choices
list of objects
Optional

Show 3 properties
Chat completion

POST
https://openrouter.ai/api/v1/chat/completions
POST
/api/v1/chat/completions

JavaScript

const url = 'https://openrouter.ai/api/v1/chat/completions';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"model":"openai/gpt-3.5-turbo"}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Successful

{
  "id": "gen-12345",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The meaning of life is a complex and subjective question..."
      }
    }
  ]
}
Send a chat completion request to a selected model. The request must contain a “messages” array. All advanced options from the base request are also supported.

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Request
This endpoint expects an object.
model
string
Required
The model ID to use. If unspecified, the user’s default is used.

messages
list of objects
Required

Show 2 properties
models
list of strings
Optional
Alternate list of models for routing overrides.

provider
object
Optional
Preferences for provider routing.


Show 1 properties
reasoning
object
Optional
Configuration for model reasoning/thinking tokens


Show 3 properties
usage
object
Optional
Whether to include usage information in the response


Show 1 properties
transforms
list of strings
Optional
List of prompt transforms (OpenRouter-only).

stream
boolean
Optional
Defaults to false
Enable streaming of results.

max_tokens
integer
Optional
Maximum number of tokens (range: [1, context_length)).

temperature
double
Optional
Sampling temperature (range: [0, 2]).

seed
integer
Optional
Seed for deterministic outputs.

top_p
double
Optional
Top-p sampling value (range: (0, 1]).

top_k
integer
Optional
Top-k sampling value (range: [1, Infinity)).

frequency_penalty
double
Optional
Frequency penalty (range: [-2, 2]).

presence_penalty
double
Optional
Presence penalty (range: [-2, 2]).

repetition_penalty
double
Optional
Repetition penalty (range: (0, 2]).

logit_bias
map from strings to doubles
Optional
Mapping of token IDs to bias values.

top_logprobs
integer
Optional
Number of top log probabilities to return.

min_p
double
Optional
Minimum probability threshold (range: [0, 1]).

top_a
double
Optional
Alternate top sampling parameter (range: [0, 1]).

Response
Successful completion

id
string
Optional
choices
list of objects
Optional

Show 1 properties
Was this page helpful?
Yes
Get a generation

GET
https://openrouter.ai/api/v1/generation
GET
/api/v1/generation

JavaScript

const url = 'https://openrouter.ai/api/v1/generation?id=id';
const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Retrieved

{
  "data": {
    "id": "id",
    "total_cost": 1.1,
    "created_at": "created_at",
    "model": "model",
    "origin": "origin",
    "usage": 1.1,
    "is_byok": true,
    "upstream_id": "upstream_id",
    "cache_discount": 1.1,
    "app_id": 1,
    "streamed": true,
    "cancelled": true,
    "provider_name": "provider_name",
    "latency": 1,
    "moderation_latency": 1,
    "generation_time": 1,
    "finish_reason": "finish_reason",
    "native_finish_reason": "native_finish_reason",
    "tokens_prompt": 1,
    "tokens_completion": 1,
    "native_tokens_prompt": 1,
    "native_tokens_completion": 1,
    "native_tokens_reasoning": 1,
    "num_media_prompt": 1,
    "num_media_completion": 1,
    "num_search_results": 1
  }
}
Returns metadata about a specific generation request

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Query parameters
id
string
Required
Response
Returns the request metadata for this generation

data
object

Show 26 properties
Was this page helpful?
Yes
No
Previous
List available models

Next
Built with

List available models

GET
https://openrouter.ai/api/v1/models
GET
/api/v1/models

JavaScript

const url = 'https://openrouter.ai/api/v1/models';
const options = {method: 'GET'};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Retrieved

{
  "data": [
    {
      "id": "id",
      "name": "name",
      "created": 1741818122,
      "description": "description",
      "architecture": {
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT"
      },
      "top_provider": {
        "is_moderated": true
      },
      "pricing": {
        "prompt": "0.0000007",
        "completion": "0.0000007",
        "image": "0",
        "request": "0",
        "input_cache_read": "0",
        "input_cache_write": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "context_length": 128000,
      "hugging_face_id": "hugging_face_id",
      "per_request_limits": {
        "key": "value"
      },
      "supported_parameters": [
        "supported_parameters"
      ]
    }
  ]
}
Returns a list of models available through the API. Note: supported_parameters is a union of all parameters supported by all providers for this model. There may not be a single provider which offers all of the listed parameters for a model.

Response
List of available models

data
list of objects

Show 11 properties
Was this page helpful?
Yes
No
Previous
List endpoints for a model

Next
Built with

List endpoints for a model

GET
https://openrouter.ai/api/v1/models/:author/:slug/endpoints
GET
/api/v1/models/:author/:slug/endpoints

JavaScript

const url = 'https://openrouter.ai/api/v1/models/author/slug/endpoints';
const options = {method: 'GET'};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Retrieved

{
  "data": {
    "id": "id",
    "name": "name",
    "created": 1.1,
    "description": "description",
    "architecture": {
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "tokenizer",
      "instruct_type": "instruct_type"
    },
    "endpoints": [
      {
        "name": "name",
        "context_length": 1.1,
        "pricing": {
          "request": "request",
          "image": "image",
          "prompt": "prompt",
          "completion": "completion"
        },
        "provider_name": "provider_name",
        "supported_parameters": [
          "supported_parameters"
        ]
      }
    ]
  }
}
Get credits

GET
https://openrouter.ai/api/v1/credits
GET
/api/v1/credits

JavaScript

const url = 'https://openrouter.ai/api/v1/credits';
const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Retrieved

{
  "data": {
    "total_credits": 1.1,
    "total_usage": 1.1
  }
}
Returns the total credits purchased and used for the authenticated user

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Response
Returns the total credits purchased and used

data
object

Show 2 properties
Was this page helpful?
Yes
No
Previous
Create a Coinbase charge

Next

Create a Coinbase charge

POST
https://openrouter.ai/api/v1/credits/coinbase
POST
/api/v1/credits/coinbase

JavaScript

const url = 'https://openrouter.ai/api/v1/credits/coinbase';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"amount":1.1,"sender":"sender","chain_id":1}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Successful

{
  "data": {
    "addresses": {
      "key": "value"
    },
    "calldata": {
      "key": "value"
    },
    "chain_id": 1,
    "sender": "sender",
    "id": "id"
  }
}
Creates and hydrates a Coinbase Commerce charge for cryptocurrency payments

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Request
This endpoint expects an object.
amount
double
Required
USD amount to charge (must be between min and max purchase limits)

sender
string
Required
Ethereum address of the sender

chain_id
integer
Required
Chain ID for the transaction

Response
Returns the calldata to fulfill the transaction

data
object

Show 5 properties
Was this page helpful?
Yes
Exchange authorization code for API key

POST
https://openrouter.ai/api/v1/auth/keys
POST
/api/v1/auth/keys

JavaScript

const url = 'https://openrouter.ai/api/v1/auth/keys';
const options = {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: '{"code":"code"}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Successful

{
  "key": "key",
  "user_id": "user_id"
}
Exchange an authorization code from the PKCE flow for a user-controlled API key

Request
This endpoint expects an object.
code
string
Required
The authorization code received from the OAuth redirect

code_verifier
string
Optional
The code verifier if code_challenge was used in the authorization request

code_challenge_method
enum
Optional
The method used to generate the code challenge

Allowed values:
S256
plain
Response
Successfully exchanged code for an API key

key
string
The API key to use for OpenRouter requests

user_id
string
Optional
User ID associated with the API key

Errors

400
Post Auth Keys Request Bad Request Error

403
Post Auth Keys Request Forbidden Error

405
Post Auth Keys Request Method Not Allowed Error
Get current API key

GET
https://openrouter.ai/api/v1/key
GET
/api/v1/key

JavaScript

const url = 'https://openrouter.ai/api/v1/key';
const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it

200
Retrieved

{
  "data": {
    "label": "My OpenRouter API Key",
    "usage": 3.25,
    "is_free_tier": false,
    "is_provisioning_key": false,
    "rate_limit": {
      "requests": 60,
      "interval": "1m"
    },
    "limit": 10,
    "limit_remaining": 6.75
  }
}
Get information on the API key associated with the current authentication session

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Response
Successfully retrieved API key information

data
object

Show 7 properties
Errors

401
Get Key Request Unauthorized Error

405
Get Key Request Method Not Allowed Error

500
Get Key Request Internal Server Error
List API keys

GET
https://openrouter.ai/api/v1/keys
GET
/api/v1/keys

JavaScript

const url = 'https://openrouter.ai/api/v1/keys';
const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Retrieved

{
  "data": [
    {
      "name": "name",
      "label": "label",
      "limit": 1.1,
      "disabled": true,
      "created_at": "created_at",
      "updated_at": "updated_at",
      "hash": "hash"
    }
  ]
}
Returns a list of all API keys associated with the account. Requires a Provisioning API key.

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Query parameters
offset
double
Optional
Offset for the API keys

include_disabled
boolean
Optional
Whether to include disabled API keys in the response

Response
List of API keys

data
list of objects

Show 7 properties
Was this page helpful?
Yes
Create API key

POST
https://openrouter.ai/api/v1/keys
POST
/api/v1/keys

JavaScript

const url = 'https://openrouter.ai/api/v1/keys';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"name":"name"}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Successful

{
  "data": {
    "name": "name",
    "label": "label",
    "limit": 1.1,
    "disabled": true,
    "created_at": "created_at",
    "updated_at": "updated_at",
    "hash": "hash"
  },
  "key": "key"
}
Creates a new API key. Requires a Provisioning API key.

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Request
This endpoint expects an object.
name
string
Required
Display name for the API key

limit
double
Optional
Optional credit limit for the key

Response
Created API key

data
object

Show 7 properties
key
string
Optional
The API key string itself. Only returned when creating a new key.

Was this page helpful?
Yes
Delete API key

DELETE
https://openrouter.ai/api/v1/keys/:hash
DELETE
/api/v1/keys/:hash

JavaScript

const url = 'https://openrouter.ai/api/v1/keys/hash';
const options = {method: 'DELETE', headers: {Authorization: 'Bearer <token>'}};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Deleted

{
  "data": {
    "success": true
  }
}
Deletes an API key. Requires a Provisioning API key.

Path parameters
hash
string
Required
The hash of the API key

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Response
Successfully deleted API key

data
object

Show 1 properties
Was this page helpful?
Yes
Update API key

PATCH
https://openrouter.ai/api/v1/keys/:hash
PATCH
/api/v1/keys/:hash

JavaScript

const url = 'https://openrouter.ai/api/v1/keys/hash';
const options = {
  method: 'PATCH',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{}'
};
try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
Try it
200
Updated

{
  "data": {
    "name": "name",
    "label": "label",
    "limit": 1.1,
    "disabled": true,
    "created_at": "created_at",
    "updated_at": "updated_at",
    "hash": "hash"
  }
}
Updates an existing API key. Requires a Provisioning API key.

Path parameters
hash
string
Required
The hash of the API key

Headers
Authorization
string
Required
Bearer authentication of the form Bearer <token>, where token is your auth token.

Request
This endpoint expects an object.
name
string
Optional
New display name for the key

disabled
boolean
Optional
Whether the key should be disabled

limit
double
Optional
New credit limit for the key

Response
Updated API key

data
object

Show 7 properties
Bring your own API Keys
OpenRouter supports both OpenRouter credits and the option to bring your own provider keys (BYOK).

When you use OpenRouter credits, your rate limits for each provider are managed by OpenRouter.

Using provider keys enables direct control over rate limits and costs via your provider account.

Your provider keys are securely encrypted and used for all requests routed through the specified provider.

Manage keys in your account settings.

The cost of using custom provider keys on OpenRouter is 5% of what the same model/provider would cost normally on OpenRouter and will be deducted from your OpenRouter credits.

Automatic Fallback
You can configure individual keys to act as fallbacks.

When “Use this key as a fallback” is enabled for a key, OpenRouter will prioritize using your credits. If it hits a rate limit or encounters a failure, it will then retry with your key.

Conversely, if “Use this key as a fallback” is disabled for a key, OpenRouter will prioritize using your key. If it hits a rate limit or encounters a failure, it will then retry with your credits.

Azure API Keys
To use Azure AI Services with OpenRouter, you’ll need to provide your Azure API key configuration in JSON format. Each key configuration requires the following fields:

{
  "model_slug": "the-openrouter-model-slug",
  "endpoint_url": "https://<resource>.services.ai.azure.com/deployments/<model-id>/chat/completions?api-version=<api-version>",
  "api_key": "your-azure-api-key",
  "model_id": "the-azure-model-id"
}

You can find these values in your Azure AI Services resource:

endpoint_url: Navigate to your Azure AI Services resource in the Azure portal. In the “Overview” section, you’ll find your endpoint URL. Make sure to append /chat/completions to the base URL. You can read more in the Azure Foundry documentation.

api_key: In the same “Overview” section of your Azure AI Services resource, you can find your API key under “Keys and Endpoint”.

model_id: This is the name of your model deployment in Azure AI Services.

model_slug: This is the OpenRouter model identifier you want to use this key for.

Since Azure supports multiple model deployments, you can provide an array of configurations for different models:

[
  {
    "model_slug": "mistralai/mistral-large",
    "endpoint_url": "https://example-project.openai.azure.com/openai/deployments/mistral-large/chat/completions?api-version=2024-08-01-preview",
    "api_key": "your-azure-api-key",
    "model_id": "mistral-large"
  },
  {
    "model_slug": "openai/gpt-4o",
    "endpoint_url": "https://example-project.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview",
    "api_key": "your-azure-api-key",
    "model_id": "gpt-4o"
  }
]

Make sure to replace the url with your own project url. Also the url should end with /chat/completions with the api version that you would like to use.

AWS Bedrock API Keys
To use Amazon Bedrock with OpenRouter, you’ll need to provide your AWS credentials in JSON format. The configuration requires the following fields:

{
  "accessKeyId": "your-aws-access-key-id",
  "secretAccessKey": "your-aws-secret-access-key",
  "region": "your-aws-region"
}

You can find these values in your AWS account:

accessKeyId: This is your AWS Access Key ID. You can create or find your access keys in the AWS Management Console under “Security Credentials” in your AWS account.

secretAccessKey: This is your AWS Secret Access Key, which is provided when you create an access key.

region: The AWS region where your Amazon Bedrock models are deployed (e.g., “us-east-1”, “us-west-2”).

Make sure your AWS IAM user or role has the necessary permissions to access Amazon Bedrock services. At minimum, you’ll need permissions for:

bedrock:InvokeModel
bedrock:InvokeModelWithResponseStream (for streaming responses)
Example IAM policy:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": "*"
    }
  ]
}

For enhanced security, we recommend creating dedicated IAM users with limited permissions specifically for use with OpenRouter.
Using MCP Servers with OpenRouter


Copy page

Use MCP Servers with OpenRouter

MCP servers are a popular way of providing LLMs with tool calling abilities, and are an alternative to using OpenAI-compatible tool calling.

By converting MCP (Anthropic) tool definitions to OpenAI-compatible tool definitions, you can use MCP servers with OpenRouter.

In this example, we’ll use Anthropic’s MCP client SDK to interact with the File System MCP, all with OpenRouter under the hood.

Note that interacting with MCP servers is more complex than calling a REST endpoint. The MCP protocol is stateful and requires session management. The example below uses the MCP client SDK, but is still somewhat complex.

First, some setup. In order to run this you will need to pip install the packages, and create a .env file with OPENAI_API_KEY set. This example also assumes the directory /Applications exists.

import asyncio
from typing import Optional
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from openai import OpenAI
from dotenv import load_dotenv
import json
load_dotenv()  # load environment variables from .env
MODEL = "anthropic/claude-3-7-sonnet"
SERVER_CONFIG = {
    "command": "npx",
    "args": ["-y",
              "@modelcontextprotocol/server-filesystem",
              f"/Applications/"],
    "env": None
}

Next, our helper function to convert MCP tool definitions to OpenAI tool definitions:

def convert_tool_format(tool):
    converted_tool = {
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description,
            "parameters": {
                "type": "object",
                "properties": tool.inputSchema["properties"],
                "required": tool.inputSchema["required"]
            }
        }
    }
    return converted_tool

And, the MCP client itself; a regrettable ~100 lines of code. Note that the SERVER_CONFIG is hard-coded into the client, but of course could be parameterized for other MCP servers.

class MCPClient:
    def __init__(self):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.openai = OpenAI(
            base_url="https://openrouter.ai/api/v1"
        )
    async def connect_to_server(self, server_config):
        server_params = StdioServerParameters(**server_config)
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        await self.session.initialize()
        # List available tools from the MCP server
        response = await self.session.list_tools()
        print("\nConnected to server with tools:", [tool.name for tool in response.tools])
        self.messages = []
    async def process_query(self, query: str) -> str:
        self.messages.append({
            "role": "user",
            "content": query
        })
        response = await self.session.list_tools()
        available_tools = [convert_tool_format(tool) for tool in response.tools]
        response = self.openai.chat.completions.create(
            model=MODEL,
            tools=available_tools,
            messages=self.messages
        )
        self.messages.append(response.choices[0].message.model_dump())
        final_text = []
        content = response.choices[0].message
        if content.tool_calls is not None:
            tool_name = content.tool_calls[0].function.name
            tool_args = content.tool_calls[0].function.arguments
            tool_args = json.loads(tool_args) if tool_args else {}
            # Execute tool call
            try:
                result = await self.session.call_tool(tool_name, tool_args)
                final_text.append(f"[Calling tool {tool_name} with args {tool_args}]")
            except Exception as e:
                print(f"Error calling tool {tool_name}: {e}")
                result = None
            self.messages.append({
                "role": "tool",
                "tool_call_id": content.tool_calls[0].id,
                "name": tool_name,
                "content": result.content
            })
            response = self.openai.chat.completions.create(
                model=MODEL,
                max_tokens=1000,
                messages=self.messages,
            )
            final_text.append(response.choices[0].message.content)
        else:
            final_text.append(content.content)
        return "\n".join(final_text)
    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")
        while True:
            try:
                query = input("\nQuery: ").strip()
                result = await self.process_query(query)
                print("Result:")
                print(result)
            except Exception as e:
                print(f"Error: {str(e)}")
    async def cleanup(self):
        await self.exit_stack.aclose()
async def main():
    client = MCPClient()
    try:
        await client.connect_to_server(SERVER_CONFIG)
        await client.chat_loop()
    finally:
        await client.cleanup()
if __name__ == "__main__":
    import sys
    asyncio.run(main())

Assembling all of the above code into mcp-client.py, you get a client that behaves as follows (some outputs truncated for brevity):

% python mcp-client.py
Secure MCP Filesystem Server running on stdio
Allowed directories: [ '/Applications' ]
Connected to server with tools: ['read_file', 'read_multiple_files', 'write_file'...]
MCP Client Started!
Type your queries or 'quit' to exit.
Query: Do I have microsoft office installed?
Result:
[Calling tool list_allowed_directories with args {}]
I can check if Microsoft Office is installed in the Applications folder:
Query: continue
Result:
[Calling tool search_files with args {'path': '/Applications', 'pattern': 'Microsoft'}]
Now let me check specifically for Microsoft Office applications:
Query: continue
Result:
I can see from the search results that Microsoft Office is indeed installed on your system.
The search found the following main Microsoft Office applications:
1. Microsoft Excel - /Applications/Microsoft Excel.app
2. Microsoft PowerPoint - /Applications/Microsoft PowerPoint.app
3. Microsoft Word - /Applications/Microsoft Word.app
4. OneDrive - /Applications/OneDrive.app (which includes Microsoft SharePoint integration)

Was this page helpful?
Yes
Provider Integration


Copy page

For Providers
If you’d like to be a model provider and sell inference on OpenRouter, fill out our form to get started.

To be eligible to provide inference on OpenRouter you must have the following:

1. List Models Endpoint
You must implement an endpoint that returns all models that should be served by OpenRouter. At this endpoint, please return a list of all available models on your platform. Below is an example of the response format:

{
  "data": [
    {
      "id": "anthropic/claude-2.0",
      "name": "Anthropic: Claude v2.0",
      "created": 1690502400,
      "description": "Anthropic's flagship model...", // Optional
      "context_length": 100000, // Required
      "max_completion_tokens": 4096, // Optional
      "quantization": "fp8", // Required
      "pricing": {
        "prompt": "0.000008", // pricing per 1 token
        "completion": "0.000024", // pricing per 1 token
        "image": "0", // pricing per 1 image
        "request": "0" // pricing per 1 request
      }
    }
  ]
}

NOTE: pricing fields are in string format to avoid floating point precision issues, and must be in USD.

Valid quantization values are: int4, int8, fp4, fp6, fp8, fp16, bf16, fp32

2. Auto Top Up or Invoicing
For OpenRouter to use the provider we must be able to pay for inference automatically. This can be done via auto top up or invoicing.

Was this page helpful?
Yes
Reasoning Tokens


Copy page

For models that support it, the OpenRouter API can return Reasoning Tokens, also known as thinking tokens. OpenRouter normalizes the different ways of customizing the amount of reasoning tokens that the model will use, providing a unified interface across different providers.

Reasoning tokens provide a transparent look into the reasoning steps taken by a model. Reasoning tokens are considered output tokens and charged accordingly.

Reasoning tokens are included in the response by default if the model decides to output them. Reasoning tokens will appear in the reasoning field of each message, unless you decide to exclude them.

Some reasoning models do not return their reasoning tokens
While most models and providers make reasoning tokens available in the response, some (like the OpenAI o-series and Gemini Flash Thinking) do not.

Controlling Reasoning Tokens
You can control reasoning tokens in your requests using the reasoning parameter:

{
  "model": "your-model",
  "messages": [],
  "reasoning": {
    // One of the following (not both):
    "effort": "high", // Can be "high", "medium", or "low" (OpenAI-style)
    "max_tokens": 2000, // Specific token limit (Anthropic-style)
    // Optional: Default is false. All models support this.
    "exclude": false // Set to true to exclude reasoning tokens from response
  }
}

The reasoning config object consolidates settings for controlling reasoning strength across different models. See the Note for each option below to see which models are supported and how other models will behave.

Max Tokens for Reasoning
Supported models
Currently supported by Anthropic and Gemini thinking models

For models that support reasoning token allocation, you can control it like this:

"max_tokens": 2000 - Directly specifies the maximum number of tokens to use for reasoning
For models that only support reasoning.effort (see below), the max_tokens value will be used to determine the effort level.

Reasoning Effort Level
Supported models
Currently supported by the OpenAI o-series and Grok models
"effort": "high" - Allocates a large portion of tokens for reasoning (approximately 80% of max_tokens)
"effort": "medium" - Allocates a moderate portion of tokens (approximately 50% of max_tokens)
"effort": "low" - Allocates a smaller portion of tokens (approximately 20% of max_tokens)
For models that only support reasoning.max_tokens, the effort level will be set based on the percentages above.

Excluding Reasoning Tokens
If you want the model to use reasoning internally but not include it in the response:

"exclude": true - The model will still use reasoning, but it won’t be returned in the response
Reasoning tokens will appear in the reasoning field of each message.

Legacy Parameters
For backward compatibility, OpenRouter still supports the following legacy parameters:

include_reasoning: true - Equivalent to reasoning: {}
include_reasoning: false - Equivalent to reasoning: { exclude: true }
However, we recommend using the new unified reasoning parameter for better control and future compatibility.

Examples
Basic Usage with Reasoning Tokens

Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
async function getResponseWithReasoning() {
  const response = await openai.chat.completions.create({
    model: 'openai/o3-mini',
    messages: [
      {
        role: 'user',
        content: "How would you build the world's tallest skyscraper?",
      },
    ],
    reasoning: {
      effort: 'high', // Use high reasoning effort
    },
  });
  console.log('REASONING:', response.choices[0].message.reasoning);
  console.log('CONTENT:', response.choices[0].message.content);
}
getResponseWithReasoning();
Using Max Tokens for Reasoning
For models that support direct token allocation (like Anthropic models), you can specify the exact number of tokens to use for reasoning:


Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
async function getResponseWithReasoning() {
  const response = await openai.chat.completions.create({
    model: 'anthropic/claude-3.7-sonnet',
    messages: [
      {
        role: 'user',
        content: "How would you build the world's tallest skyscraper?",
      },
    ],
    reasoning: {
      max_tokens: 2000, // Allocate 2000 tokens (or approximate effort) for reasoning
    },
  });
  console.log('REASONING:', response.choices[0].message.reasoning);
  console.log('CONTENT:', response.choices[0].message.content);
}
getResponseWithReasoning();
Excluding Reasoning Tokens from Response
If you want the model to use reasoning internally but not include it in the response:


Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
async function getResponseWithReasoning() {
  const response = await openai.chat.completions.create({
    model: 'deepseek/deepseek-r1',
    messages: [
      {
        role: 'user',
        content: "How would you build the world's tallest skyscraper?",
      },
    ],
    reasoning: {
      effort: 'high',
      exclude: true, // Use reasoning but don't include it in the response
    },
  });
  console.log('REASONING:', response.choices[0].message.reasoning);
  console.log('CONTENT:', response.choices[0].message.content);
}
getResponseWithReasoning();
Advanced Usage: Reasoning Chain-of-Thought
This example shows how to use reasoning tokens in a more complex workflow. It injects one model’s reasoning into another model to improve its response quality:


Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey,
});
async function doReq(model, content, reasoningConfig) {
  const payload = {
    model,
    messages: [{ role: 'user', content }],
    stop: '</think>',
    ...reasoningConfig,
  };
  return openai.chat.completions.create(payload);
}
async function getResponseWithReasoning() {
  const question = 'Which is bigger: 9.11 or 9.9?';
  const reasoningResponse = await doReq(
    'deepseek/deepseek-r1',
    `${question} Please think this through, but don't output an answer`,
  );
  const reasoning = reasoningResponse.choices[0].message.reasoning;
  // Let's test! Here's the naive response:
  const simpleResponse = await doReq('openai/gpt-4o-mini', question);
  console.log(simpleResponse.choices[0].message.content);
  // Here's the response with the reasoning token injected:
  const content = `${question}. Here is some context to help you: ${reasoning}`;
  const smartResponse = await doReq('openai/gpt-4o-mini', content);
  console.log(smartResponse.choices[0].message.content);
}
getResponseWithReasoning();
Provider-Specific Reasoning Implementation
Anthropic Models with Reasoning Tokens
The latest Claude models, such as anthropic/claude-3.7-sonnet, support working with and returning reasoning tokens.

You can enable reasoning on Anthropic models in two ways:

Using the :thinking variant suffix (e.g., anthropic/claude-3.7-sonnet:thinking). The thinking variant defaults to high effort.
Using the unified reasoning parameter with either effort or max_tokens
Reasoning Max Tokens for Anthropic Models
When using Anthropic models with reasoning:

When using the reasoning.max_tokens parameter, that value is used directly with a minimum of 1024 tokens.
When using the :thinking variant suffix or the reasoning.effort parameter, the budget_tokens are calculated based on the max_tokens value.
The reasoning token allocation is capped at 32,000 tokens maximum and 1024 tokens minimum. The formula for calculating the budget_tokens is: budget_tokens = max(min(max_tokens * {effort_ratio}, 32000), 1024)

effort_ratio is 0.8 for high effort, 0.5 for medium effort, and 0.2 for low effort.

Important: max_tokens must be strictly higher than the reasoning budget to ensure there are tokens available for the final response after thinking.

Token Usage and Billing
Please note that reasoning tokens are counted as output tokens for billing purposes. Using reasoning tokens will increase your token usage but can significantly improve the quality of model responses.

Examples with Anthropic Models
Example 1: Streaming mode with reasoning tokens

Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey,
});
async function chatCompletionWithReasoning(messages) {
  const response = await openai.chat.completions.create({
    model: 'anthropic/claude-3.7-sonnet',
    messages,
    maxTokens: 10000,
    reasoning: {
      maxTokens: 8000, // Directly specify reasoning token budget
    },
    stream: true,
  });
  return response;
}
Usage Accounting


Copy page

The OpenRouter API provides built-in Usage Accounting that allows you to track AI model usage without making additional API calls. This feature provides detailed information about token counts, costs, and caching status directly in your API responses.

Usage Information
When enabled, the API will return detailed usage information including:

Prompt and completion token counts using the model’s native tokenizer
Cost in credits
Reasoning token counts (if applicable)
Cached token counts (if available)
This information is included in the last SSE message for streaming responses, or in the complete response for non-streaming requests.

Enabling Usage Accounting
You can enable usage accounting in your requests by including the usage parameter:

{
  "model": "your-model",
  "messages": [],
  "usage": {
    "include": true
  }
}

Response Format
When usage accounting is enabled, the response will include a usage object with detailed token information:

{
  "object": "chat.completion.chunk",
  "usage": {
    "completion_tokens": 2,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    },
    "cost": 197,
    "prompt_tokens": 194,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "total_tokens": 196
  }
}

Performance Impact
Enabling usage accounting will add a few hundred milliseconds to the last response as the API calculates token counts and costs. This only affects the final message and does not impact overall streaming performance.

Benefits
Efficiency: Get usage information without making separate API calls
Accuracy: Token counts are calculated using the model’s native tokenizer
Transparency: Track costs and cached token usage in real-time
Detailed Breakdown: Separate counts for prompt, completion, reasoning, and cached tokens
Best Practices
Enable usage tracking when you need to monitor token consumption or costs
Account for the slight delay in the final response when usage accounting is enabled
Consider implementing usage tracking in development to optimize token usage before production
Use the cached token information to optimize your application’s performance
Alternative: Getting Usage via Generation ID
You can also retrieve usage information asynchronously by using the generation ID returned from your API calls. This is particularly useful when you want to fetch usage statistics after the completion has finished or when you need to audit historical usage.

To use this method:

Make your chat completion request as normal
Note the id field in the response
Use that ID to fetch usage information via the /generation endpoint
For more details on this approach, see the Get a Generation documentation.

Examples
Basic Usage with Token Tracking

Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
async function getResponseWithUsage() {
  const response = await openai.chat.completions.create({
    model: 'anthropic/claude-3-opus',
    messages: [
      {
        role: 'user',
        content: 'What is the capital of France?',
      },
    ],
    extra_body: {
      usage: {
        include: true,
      },
    },
  });
  console.log('Response:', response.choices[0].message.content);
  console.log('Usage Stats:', response.usage);
}
getResponseWithUsage();
Streaming with Usage Information
This example shows how to handle usage information in streaming mode:


Python

TypeScript

import OpenAI from 'openai';
const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
async function chatCompletionWithUsage(messages) {
  const response = await openai.chat.completions.create({
    model: 'anthropic/claude-3-opus',
    messages,
    usage: {
      include: true,
    },
    stream: true,
  });
  return response;
}
(async () => {
  for await (const chunk of chatCompletionWithUsage([
    { role: 'user', content: 'Write a haiku about Paris.' },
  ])) {
    if (chunk.usage) {
      console.log('\nUsage Statistics:');
      console.log(`Total Tokens: ${chunk.usage.total_tokens}`);
      console.log(`Prompt Tokens: ${chunk.usage.prompt_tokens}`);
      console.log(`Completion Tokens: ${chunk.usage.completion_tokens}`);
      console.log(`Cost: ${chunk.usage.cost} credits`);
    } else if (chunk.choices[0].delta.content) {
      process.stdout.write(chunk.choices[0].delta.content);
    }
  }
})();
Was this page helpful?
Yes

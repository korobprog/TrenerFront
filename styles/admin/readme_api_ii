# Langdock API Documentation

## Содержание

1. [Completion API](#completion-api)
   - [OpenAI Chat completion](#openai-chat-completion)
   - [Anthropic Messages](#anthropic-messages)
   - [Codestral](#codestral)
2. [Embedding API](#embedding-api)
   - [OpenAI Embeddings](#openai-embeddings)
3. [Assistant API](#assistant-api)
4. [Knowledge Folder API](#knowledge-folder-api)

---

## Completion API

### OpenAI Chat completion

**Endpoint:** `POST /openai/{region}/v1/chat/completions`

Создает ответ модели для заданной чат-беседы с использованием модели OpenAI.

#### Пример запроса:

```javascript
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'},
  body: '{"model":"gpt-4o-mini","messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a short poem about cats."}]}'
};

fetch('https://api.langdock.com/openai/{region}/v1/chat/completions', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));
```

#### Пример ответа:

```json
{
  "choices": [
    {
      "message": {
        "content": "In moonlit shadows soft they prowl,\nWith eyes aglow in night's dark cowl.",
        "role": "assistant"
      },
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null
    }
  ],
  "created": 1721722200,
  "id": "chatcmpl-8o4sq3sSzGVqS0aQyjlXuuEGVZnSj",
  "model": "gpt-4o-2024-05-13",
  "object": "chat.completion",
  "system_fingerprint": "fp_asd28019bf",
  "usage": {
    "completion_tokens": 34,
    "prompt_tokens": 14,
    "total_tokens": 48
  }
}
```

#### Важные параметры:

- **model**: Поддерживаются модели o3-mini, o1-preview, gpt-4o, gpt-4o-mini, gpt-4 и gpt-35-turbo.
- **messages**: Массив сообщений, составляющих беседу.
- **temperature**: Значение от 0 до 2. Более высокие значения делают вывод более случайным.
- **max_tokens**: Максимальное количество токенов для генерации.

#### Ограничения:

- Лимит запросов: 500 RPM (запросов в минуту)
- Лимит токенов: 60,000 TPM (токенов в минуту)

#### Использование с библиотеками:

**Python (OpenAI):**
```python
from openai import OpenAI
client = OpenAI(
  base_url="https://api.langdock.com/openai/eu/v1",
  api_key="YOUR_LANGDOCK_API_KEY"
)

completion = client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
    {"role": "user", "content": "Write a short poem about cats."}
  ]
)

print(completion.choices[0].message.content)
```

**Node.js (Vercel AI SDK):**
```javascript
import { streamText } from "ai";
import { createOpenAI } from "@ai-sdk/openai";

const langdockProvider = createOpenAI({
  baseURL: "https://api.langdock.com/openai/eu/v1",
  apiKey: "YOUR_LANGDOCK_API_KEY",
});

const result = await streamText({
  model: langdockProvider("gpt-4o-mini"),
  prompt: "Write a short poem about cats",
});

for await (const textPart of result.textStream) {
  process.stdout.write(textPart);
}
```

---

### Anthropic Messages

**Endpoint:** `POST /anthropic/{region}/v1/messages`

Создает ответ модели на основе структурированного списка входных сообщений с использованием API Anthropic.

#### Пример запроса:

```javascript
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'},
  body: '{"max_tokens":1024,"messages":[{"content":"Write a haiku about cats.","role":"user"}],"model":"claude-3-haiku-20240307"}'
};

fetch('https://api.langdock.com/anthropic/{region}/v1/messages', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));
```

#### Пример ответа:

```json
[
  {
    "content": [
      {
        "text": "Here is a haiku about cats:\n\nFeline grace and charm,\nPurring softly by the fire,\nCats reign supreme.",
        "type": "text"
      }
    ],
    "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",
    "model": "claude-3-haiku-20240307",
    "role": "assistant",
    "stop_reason": "end_turn",
    "stop_sequence": null,
    "type": "message",
    "usage": {
      "input_tokens": 14,
      "output_tokens": 35
    }
  }
]
```

#### Поддерживаемые модели:

- **EU регион**: claude-3-7-sonnet-20250219, claude-3-5-sonnet-20240620, claude-3-sonnet-20240229, claude-3-haiku-20240307
- **US регион**: claude-3-5-sonnet-20240620, claude-3-haiku-20240307, claude-3-opus-20240229

#### Важные параметры:

- **model**: Модель для использования.
- **messages**: Входные сообщения.
- **max_tokens**: Максимальное количество токенов для генерации.
- **temperature**: Значение от 0 до 1. Более высокие значения делают вывод более случайным.

#### Использование с библиотеками:

**Python (Anthropic):**
```python
from anthropic import Anthropic
client = Anthropic(
  base_url="https://api.langdock.com/anthropic/eu/",
  api_key="YOUR_LANGDOCK_API_KEY"
)

message = client.messages.create(
  model="claude-3-haiku-20240307",
  messages=[
    { "role": "user", "content": "Write a haiku about cats" }
  ],
  max_tokens=1024,
)

print(message.content[0].text)
```

**Node.js (Vercel AI SDK):**
```javascript
import { generateText } from "ai";
import { createAnthropic } from "@ai-sdk/anthropic";

const langdockProvider = createAnthropic({
  baseURL: "https://api.langdock.com/anthropic/eu/v1",
  apiKey: "YOUR_LANGDOCK_API_KEY",
});

const result = await generateText({
  model: langdockProvider("claude-3-haiku-20240307"),
  prompt: "Write a haiku about cats",
});

console.log(result.text);
```

---

### Codestral

**Endpoint:** `POST /mistral/{region}/v1/fim/completions`

Генерация кода с использованием модели Codestral от Mistral.

#### Пример запроса:

```javascript
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'},
  body: '{"model":"codestral-2405","prompt":"function removeSpecialCharactersWithRegex(str: string) {","max_tokens":64}'
};

fetch('https://api.langdock.com/mistral/{region}/v1/fim/completions', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));
```

#### Пример ответа:

```json
{
  "data": "asd",
  "id": "245c52bc936f53ba90327800c73d1c3e",
  "object": "chat.completion",
  "model": "codestral",
  "usage": {
    "prompt_tokens": 16,
    "completion_tokens": 102,
    "total_tokens": 118
  },
  "created": 1732902806,
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "\n // Use a regular expression to match any non-alphanumeric character and replace it with an empty string\n return str.replace(/[^a-zA-Z0-9]/g, '');\n}\n\n// Test the function\nconst inputString = \"Hello, World! 123\";\nconst outputString = removeSpecialCharactersWithRegex(inputString);\nconsole.log(outputString); // Output: \"HelloWorld123\"",
        "prefix": false,
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ]
}
```

#### Важные параметры:

- **model**: Поддерживается модель codestral-2405.
- **prompt**: Текст/код для завершения.
- **max_tokens**: Максимальное количество токенов для генерации.
- **temperature**: Значение от 0 до 1.5. Более высокие значения делают вывод более случайным.

#### Использование с Continue AI Code Assistant:

Continue - это ассистент кода с открытым исходным кодом, доступный как расширение VS Code и JetBrains. Пример конфигурации:

```json
{
  "models": [
    {
      "title": "GPT-4o",
      "provider": "openai",
      "model": "gpt-4o",
      "apiKey": "YOUR_LANGDOCK_API_KEY",
      "apiBase": "https://api.langdock.com/openai/eu/v1"
    },
    {
      "title": "Claude 3.5 Sonnet",
      "provider": "anthropic",
      "model": "claude-3-5-sonnet-20240620",
      "apiKey": "YOUR_LANGDOCK_API_KEY",
      "apiBase": "https://api.langdock.com/anthropic/eu/v1"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Codestral",
    "provider": "mistral",
    "model": "codestral-2405",
    "apiKey": "YOUR_LANGDOCK_API_KEY",
    "apiBase": "https://api.langdock.com/mistral/eu/v1"
  }
}
```

---

## Embedding API

### OpenAI Embeddings

**Endpoint:** `POST /openai/{region}/v1/embeddings`

Создает эмбеддинги для текста с использованием моделей эмбеддинга OpenAI.

#### Пример запроса:

```javascript
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'},
  body: '{"model":"text-embedding-ada-002","input":"The quick brown fox jumps over the lazy dog","encoding_format":"float"}'
};

fetch('https://api.langdock.com/openai/{region}/v1/embeddings', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));
```

#### Пример ответа:

```json
{
  "data": [
    {
      "embedding": [
        0.0023064255,
        -0.009327292,
        "..."
      ],
      "index": 0,
      "object": "embedding"
    }
  ],
  "model": "text-embedding-ada-002",
  "object": "list",
  "usage": {
    "prompt_tokens": 9,
    "total_tokens": 9
  }
}
```

#### Важные параметры:

- **model**: Поддерживается модель text-embedding-ada-002.
- **input**: Входной текст для получения эмбеддингов.
- **encoding_format**: Поддерживаются форматы float и base64.

#### Использование с библиотеками:

**Python (OpenAI):**
```python
from openai import OpenAI
client = OpenAI(
  base_url="https://api.langdock.com/openai/eu/v1",
  api_key="YOUR_LANGDOCK_API_KEY"
)

embedding = client.embeddings.create(
  model="text-embedding-ada-002",
  input="The quick brown fox jumps over the lazy dog",
  encoding_format="float"
)

print(embedding.data[0].embedding)
```

**Node.js (Vercel AI SDK):**
```javascript
import { createOpenAI } from "@ai-sdk/openai";

const langdockProvider = createOpenAI({
  baseURL: "https://api.langdock.com/openai/eu/v1",
  apiKey: "YOUR_LANGDOCK_API_KEY",
});

const response = await langdockProvider.embeddings.create({
  model: "text-embedding-ada-002",
  input: "The quick brown fox jumps over the lazy dog",
  encoding_format: "float",
});

console.log(response.data[0].embedding);
```

---

## Assistant API

Включает следующие эндпоинты:
- Sharing Assistants with API Keys
- Assistant API
- Models for Assistant API
- Upload Attachment API

---

## Knowledge Folder API

Включает следующие эндпоинты:
- Upload a file to a knowledge folder
- Update a file in a knowledge folder
- Retrieve files from a knowledge folder
- Delete a file from a knowledge folder
- Search through all files in data folders shared with the API Key

---

## Общая информация

### Авторизация

Для всех API требуется API-ключ в формате Bearer-токена:
```
Authorization: Bearer YOUR_API_KEY
```

### Регионы

Доступные регионы:
- eu (Европа)
- us (США, доступен не для всех API)

### Ограничения

- Лимит запросов: 500 RPM (запросов в минуту)
- Лимит токенов: 60,000 TPM (токенов в минуту)

Лимиты определяются на уровне рабочего пространства, а не на уровне API-ключа. Каждая модель имеет свой собственный лимит. При превышении лимита вы получите ответ 429 Too Many Requests.
